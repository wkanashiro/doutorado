{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6825e50e",
   "metadata": {},
   "source": [
    "# Computing the PCA of a Foreground Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89fa7dd-f5c8-42b7-896c-8287d4e97105",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Let's start by loading some pre-requisites and checking the DINOv3 repository location:\n",
    "- `local` if `DINOV3_LOCATION` environment variable was set to work with a local version of DINOv3 repository;\n",
    "- `github` if the code should be loaded via torch hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ebd98e9-13eb-4e7a-a72b-27839dd463d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "\n",
    "import random\n",
    "import tqdm # Progress bar\n",
    "from sklearn.metrics import pairwise_distances # Isso aqui é para calcular a distancia média entre os patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a42963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 location set to /home/lades/computer_vision/wesley/dino-soja/dinov3\n",
      "Loading DINOv3 model dinov3_vitl16...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (rope_embed): RopePositionEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-23): 24 x SelfAttentionBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SelfAttention(\n",
       "        (qkv): LinearKMaskedBias(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (local_cls_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DINOV3_GITHUB_LOCATION = \"/home/lades/computer_vision/wesley/dino-soja/dinov3\"\n",
    "\n",
    "if os.getenv(\"DINOV3_LOCATION\") is not None:\n",
    "    DINOV3_LOCATION = os.getenv(\"DINOV3_LOCATION\")\n",
    "else:\n",
    "    DINOV3_LOCATION = DINOV3_GITHUB_LOCATION\n",
    "\n",
    "print(f\"DINOv3 location set to {DINOV3_LOCATION}\")\n",
    "\n",
    "# examples of available DINOv3 models:\n",
    "MODEL_DINOV3_VITS = \"dinov3_vits16\"\n",
    "MODEL_DINOV3_VITSP = \"dinov3_vits16plus\"\n",
    "MODEL_DINOV3_VITB = \"dinov3_vitb16\"\n",
    "MODEL_DINOV3_VITL = \"dinov3_vitl16\"\n",
    "MODEL_DINOV3_VITHP = \"dinov3_vith16plus\"\n",
    "MODEL_DINOV3_VIT7B = \"dinov3_vit7b16\"\n",
    "\n",
    "MODEL_NAME = MODEL_DINOV3_VITL\n",
    "\n",
    "print(f\"Loading DINOv3 model {MODEL_NAME}...\")\n",
    "\n",
    "model = torch.hub.load(\n",
    "    repo_or_dir=DINOV3_LOCATION,\n",
    "    model=MODEL_NAME,\n",
    "    source=\"local\",\n",
    "    weights=\"https://dinov3.llamameta.net/dinov3_vitl16/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiamVkbGhoemY3bnlpYmJ1NnVhdmJ2NGtrIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NTcxNzAwMzh9fX1dfQ__&Signature=YktyLH8aRk9MBqJe9X6LfVwzezdJo51sVHwKx29n7BTo88T8QR4HHDj9kCyi2wVyUsI4qDVXoJWfHDdbbkxiEPqXukQUDVtWq25HG5ODC8JP-%7ENDbRUPQYkPUHER6ssK9WK5K4Nva6EdiBtIKqMm9G3RMh1uFA86wGz4X4FvgDiaRh4aCWsM5jLg3Gsvr1QIFoPpgAO%7EOnYYG-wxj1VRnrY32Wm6OoNw61M96kgfPXNxEFwYphBu2ImJykrvgc0Yea1J2jV3FwECfDmdVftQ3okThpbxwPJ-wsAxDXc2z2-bJaPQowafr8afLKO2hFl57iMiDQELW2DDS9GYgvpw7Q__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=783982447506725\"\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8428926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 16\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "SATELLITE_MEAN = (0.430, 0.411, 0.296)\n",
    "SATELLITE_STD = (0.213, 0.156, 0.143)\n",
    "\n",
    "MODEL_TO_NUM_LAYERS = {\n",
    "    MODEL_DINOV3_VITS: 12,\n",
    "    MODEL_DINOV3_VITSP: 12,\n",
    "    MODEL_DINOV3_VITB: 12,\n",
    "    MODEL_DINOV3_VITL: 24,\n",
    "    MODEL_DINOV3_VITHP: 32,\n",
    "    MODEL_DINOV3_VIT7B: 40,\n",
    "}\n",
    "\n",
    "n_layers = MODEL_TO_NUM_LAYERS[MODEL_NAME]\n",
    "    \n",
    "def load_image(path: str) -> Image:\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "# image resize transform to dimensions divisible by patch size\n",
    "def resize_transform(\n",
    "    mask_image: Image,\n",
    "    image_size: int = IMAGE_SIZE,\n",
    "    patch_size: int = PATCH_SIZE,\n",
    ") -> torch.Tensor:\n",
    "    w, h = mask_image.size\n",
    "    h_patches = int(image_size / patch_size)\n",
    "    w_patches = int((w * image_size) / (h * patch_size))\n",
    "    return TF.to_tensor(TF.resize(mask_image, (h_patches * patch_size, w_patches * patch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a069633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using image /home/lades/computer_vision/wesley/dataset/daninhas_multiclasse/DATASET_CARURU/rgb/2025-09-17_TH134_06-05-2025_c_mask_14_8807_24073.jpg\n",
      "Using label /home/lades/computer_vision/wesley/dataset/daninhas_multiclasse/DATASET_CARURU/labels/2025-09-17_TH134_06-05-2025_c_mask_14_8807_24073.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   1, 255], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obter imagem aleatória do dataset\n",
    "dataset_path = \"/home/lades/computer_vision/wesley/dataset/daninhas_multiclasse/DATASET_CARURU/rgb/\"\n",
    "image_uri = f\"{dataset_path}{random.choice(os.listdir(dataset_path))}\"\n",
    "\n",
    "print(f\"Using image {image_uri}\")\n",
    "\n",
    "# Obtém o respectivo label\n",
    "label_uri = image_uri.replace(\"rgb\", \"labels\").replace(\".jpg\", \".png\")\n",
    "label = np.array(Image.open(label_uri))\n",
    "\n",
    "print(f\"Using label {label_uri}\")\n",
    "np.unique(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82cc18e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_tensores(label, image_uri):\n",
    "    image = load_image(image_uri)\n",
    "    image_resized = resize_transform(image)\n",
    "    image_resized_norm = TF.normalize(image_resized, mean=SATELLITE_MEAN, std=SATELLITE_STD)\n",
    "\n",
    "    label_mask = (label == 1).astype(np.float32)\n",
    "    label_mask = signal.convolve2d(label_mask, np.ones((PATCH_SIZE, PATCH_SIZE)), mode='valid')[::PATCH_SIZE, ::PATCH_SIZE]\n",
    "    fg_score_mf = torch.from_numpy(label_mask > 0.5)\n",
    "\n",
    "    h_patches, w_patches = image_resized_norm.shape[1] // PATCH_SIZE, image_resized_norm.shape[2] // PATCH_SIZE\n",
    "    #print(f\"Image size: {image_resized_norm.shape}, patches: {h_patches}x{w_patches}\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
    "            feats = model.get_intermediate_layers(image_resized_norm.unsqueeze(0).cuda(), n=range(n_layers), reshape=True, norm=True)\n",
    "            x = feats[-1].squeeze().detach().cpu()\n",
    "            dim = x.shape[0]\n",
    "            x = x.view(dim, -1).permute(1, 0)\n",
    "            \n",
    "    x.shape, fg_score_mf.shape\n",
    "\n",
    "    # Dividir x em foreground e background, considerando a label_mask\n",
    "    foreground_x = x[fg_score_mf.view(-1) > 0.5]\n",
    "    background_x = x[fg_score_mf.view(-1) <= 0.5]\n",
    "\n",
    "    #print(f'Foreground: {foreground_x.shape}, Background: {background_x.shape}')\n",
    "\n",
    "    return x, foreground_x, background_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b22b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_pca(label, image_uri): \n",
    "    # Cria a máscara de foreground (tudo que estiver no fundo preto que seja diferente de branco)\n",
    "    label_mask = (label == 1).astype(np.float32)\n",
    "    label_mask = signal.convolve2d(label_mask, np.ones((PATCH_SIZE, PATCH_SIZE)), mode='valid')[::PATCH_SIZE, ::PATCH_SIZE]\n",
    "    fg_score_mf = torch.from_numpy(label_mask > 0.5)\n",
    "\n",
    "    image = load_image(image_uri)\n",
    "    image_resized = resize_transform(image)\n",
    "    image_resized_norm = TF.normalize(image_resized, mean=SATELLITE_MEAN, std=SATELLITE_STD)\n",
    "\n",
    "    h_patches, w_patches = image_resized_norm.shape[1] // PATCH_SIZE, image_resized_norm.shape[2] // PATCH_SIZE\n",
    "    print(f\"Image size: {image_resized_norm.shape}, patches: {h_patches}x{w_patches}\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
    "            feats = model.get_intermediate_layers(image_resized_norm.unsqueeze(0).cuda(), n=range(n_layers), reshape=True, norm=True)\n",
    "            x = feats[-1].squeeze().detach().cpu()\n",
    "            dim = x.shape[0]\n",
    "            x = x.view(dim, -1).permute(1, 0)\n",
    "            \n",
    "    fg_patches = x\n",
    "\n",
    "    pca = PCA(n_components=3, whiten=True)\n",
    "    pca.fit(fg_patches)\n",
    "\n",
    "    # apply the PCA, and then reshape\n",
    "    projected_image = torch.from_numpy(pca.transform(x.numpy())).view(h_patches, w_patches, 3)\n",
    "\n",
    "    # multiply by 2.0 and pass through a sigmoid to get vibrant colors \n",
    "    projected_image = torch.nn.functional.sigmoid(projected_image.mul(2.0)).permute(2, 0, 1)\n",
    "\n",
    "    # mask the background using the fg_score_mf\n",
    "    projected_foreground_image = projected_image * (fg_score_mf.unsqueeze(0) > 0.5)\n",
    "    \n",
    "    projected_background_image = projected_image * (fg_score_mf.unsqueeze(0) <= 0.5)\n",
    "\n",
    "    return projected_image, projected_foreground_image, projected_background_image\n",
    "\n",
    "def plotar_imagens(image_uri, projected_image, projected_foreground_image, label_mask, label_uri):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.imshow(np.array(Image.open(image_uri)))\n",
    "    plt.title(\"Imagem original\")\n",
    "    plt.subplot(1, 5, 2)\n",
    "    plt.imshow(projected_image.permute(1, 2, 0))\n",
    "    plt.title(\"PCA da imagem projetada\")\n",
    "    plt.subplot(1, 5, 3)\n",
    "    plt.imshow(projected_foreground_image.permute(1, 2, 0))\n",
    "    plt.title(\"PCA da imagem projetada + mask\")\n",
    "    plt.subplot(1, 5, 4)\n",
    "    plt.imshow(np.array(Image.fromarray(label_mask)))\n",
    "    plt.title(\"PCA da mask label (GT)\")\n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.imshow(Image.open(label_uri))\n",
    "    plt.title(\"Label (GT)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Exibir label_mask com os valores de cada bloquinho\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(label_mask)\n",
    "    for i in range(label_mask.shape[0]):\n",
    "        for j in range(label_mask.shape[1]):\n",
    "            plt.text(j, i, f\"{label_mask[i, j]:.1f}\", ha='center', va='center', color='white')\n",
    "    plt.show()\n",
    "\n",
    "    # Exibir projected_image com os valores de cada bloquinho\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(projected_image.permute(1, 2, 0))\n",
    "    for i in range(projected_image.shape[1]):\n",
    "        for j in range(projected_image.shape[2]):\n",
    "            plt.text(j, i, f\"{projected_image[0, i, j]:.1f}\", ha='center', va='center', color='white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936f236",
   "metadata": {},
   "source": [
    "### Experimento\n",
    "Neste experimento, irei considerar o mask label da respectiva imagem, para separar os patches do PCA da imagem que estiverem dentro da mask label da imagem. Isso sera o foreground e o restante, background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4894d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrupar_tensores(dataset_path): # dataset_path deve ser o path do dataset (ex: .../DATASET_CARURU/)\n",
    "    dataset_path_rgb = f\"{dataset_path}/rgb/\"\n",
    "    dataset_path_label = f\"{dataset_path}/labels/\" # Extensão png\n",
    "    \n",
    "    lista_imagens = [f\"{dataset_path_rgb}{file}\" for file in os.listdir(dataset_path_rgb) if file.endswith('.jpg')]\n",
    "    lista_labels = [f\"{dataset_path_label}{file}\" for file in os.listdir(dataset_path_label) if file.endswith('.png')]\n",
    "    \n",
    "    lista_imagens.sort()\n",
    "    lista_labels.sort()\n",
    "\n",
    "    print(f\"Foram encontrados {len(lista_imagens)} imagens e {len(lista_labels)} labels.\")\n",
    "\n",
    "    x_foreground = []\n",
    "    x_background = []\n",
    "\n",
    "    # Iterar sobre as imagens utilizando tqdm para mostrar o progresso\n",
    "    for image_uri, label_uri in tqdm.tqdm(zip(lista_imagens, lista_labels), total=len(lista_imagens)):\n",
    "        label = np.array(Image.open(label_uri))\n",
    "        x, foreground, background = extrair_tensores(label, image_uri)\n",
    "        x_foreground.append(foreground)\n",
    "        x_background.append(background)\n",
    "\n",
    "    all_foreground = torch.cat(x_foreground, dim=0)\n",
    "    all_background = torch.cat(x_background, dim=0)\n",
    "    \n",
    "    print(f'Total Foreground: {all_foreground.shape}, Total Background: {all_background.shape}')\n",
    "    \n",
    "    return all_foreground, all_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e06f7534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CARURU', 'DATASET_CARURU', 'CAR'),\n",
       " ('GRAMINEA_PORTE_ALTO', 'DATASET_GRAMINEA_PORTE_ALTO', 'GPA'),\n",
       " ('GRAMINEA_PORTE_BAIXO', 'DATASET_GRAMINEA_PORTE_BAIXO', 'GPB'),\n",
       " ('MAMONA', 'DATASET_MAMONA', 'MAM'),\n",
       " ('OUTRAS_FOLHAS_LARGAS', 'DATASET_OUTRAS_FOLHAS_LARGAS', 'OFL'),\n",
       " ('TREPADEIRA', 'DATASET_TREPADEIRA', 'TRE')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"/home/lades/computer_vision/wesley/dataset/daninhas_multiclasse/\"\n",
    "\n",
    "# Obter os nomes das classes, pelo sufixo DATASET_\n",
    "dir_classes = [d for d in os.listdir(dataset_path) if d.startswith(\"DATASET_\")]\n",
    "\n",
    "classes = []\n",
    "for dir_class in dir_classes:\n",
    "    class_name = dir_class.split(\"DATASET_\")[-1]\n",
    "    # nickname sera as 3 primeiras letras (se a classe tiver somente uma palavra); ou as iniciais das palavras (se tiver mais de uma palavra)\n",
    "    if \"_\" in class_name:\n",
    "        nickname = \"\".join([word[0] for word in class_name.split(\"_\")]).upper()\n",
    "    else:\n",
    "        nickname = class_name[:3].upper()\n",
    "    classes.append((class_name, dir_class, nickname))\n",
    "\n",
    "classes.sort()\n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f0d57",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mO Kernel deu pane ao executar o código na célula atual ou em uma célula anterior. \n",
      "\u001b[1;31mAnalise o código nas células para identificar uma possível causa da pane. \n",
      "\u001b[1;31mClique <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqui</a> para obter mais informações. \n",
      "\u001b[1;31mConsulte Jupyter <a href='command:jupyter.viewOutput'>log</a> para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "def calcular_distancias_e_desvio_padrao(all_x_foreground, all_x_background, \n",
    "                                 max_samples_fg=5000, max_samples_bg=10000):\n",
    "    \"\"\"\n",
    "    Calcula distâncias usando amostragem para economizar memória\n",
    "    \"\"\"\n",
    "    print(f\"Original - FG: {all_x_foreground.shape[0]:,}, BG: {all_x_background.shape[0]:,}\")\n",
    "    \n",
    "    if all_x_foreground.shape[0] > max_samples_fg:\n",
    "        # Amostrar foreground\n",
    "        indices_fg = torch.randperm(all_x_foreground.shape[0])[:max_samples_fg]\n",
    "        fg_sample = all_x_foreground[indices_fg]\n",
    "        #print(f\"Amostrando foreground: {max_samples_fg:,} patches\")\n",
    "    else:\n",
    "        fg_sample = all_x_foreground\n",
    "        \n",
    "    if all_x_background.shape[0] > max_samples_bg:\n",
    "        # Amostrar background\n",
    "        indices_bg = torch.randperm(all_x_background.shape[0])[:max_samples_bg]\n",
    "        bg_sample = all_x_background[indices_bg]\n",
    "        #print(f\"Amostrando background: {max_samples_bg:,} patches\")\n",
    "    else:\n",
    "        bg_sample = all_x_background\n",
    "    \n",
    "    print(f\"Calculando distâncias: {fg_sample.shape[0]:,} x {fg_sample.shape[0]:,} (intra)\")\n",
    "    print(f\"Calculando distâncias: {fg_sample.shape[0]:,} x {bg_sample.shape[0]:,} (inter)\")\n",
    "    \n",
    "    # Calcular distâncias nas amostras\n",
    "    mean_dist_intra = pairwise_distances(fg_sample).mean()\n",
    "    mean_dist_inter = pairwise_distances(fg_sample, bg_sample).mean()\n",
    "    std_intra = pairwise_distances(fg_sample).std()\n",
    "    std_inter = pairwise_distances(fg_sample, bg_sample).std()\n",
    "\n",
    "    return mean_dist_intra, mean_dist_inter, std_intra, std_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5795dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando classe CARURU – CAR...\n",
      "Foram encontrados 97 imagens e 97 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:13<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Foreground: torch.Size([5205, 1024]), Total Background: torch.Size([19627, 1024])\n",
      "Original - FG: 5,205, BG: 19,627\n",
      "Calculando distâncias: 2,602 x 2,602 (intra)\n",
      "Calculando distâncias: 2,602 x 9,813 (inter)\n",
      "Distância média CAR_CAR: 5.203938007354736, CAR_not_CAR: 5.625357151031494\n",
      "Desvio padrão CAR_CAR: 0.6614007949829102, CAR_not_CAR: 0.6321923136711121\n",
      "Processando classe GRAMINEA_PORTE_ALTO – GPA...\n",
      "Foram encontrados 2726 imagens e 2726 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2726/2726 [02:37<00:00, 17.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Foreground: torch.Size([27369, 1024]), Total Background: torch.Size([670487, 1024])\n",
      "Original - FG: 27,369, BG: 670,487\n",
      "Calculando distâncias: 13,684 x 13,684 (intra)\n",
      "Calculando distâncias: 13,684 x 335,243 (inter)\n",
      "Distância média GPA_GPA: 6.252010822296143, GPA_not_GPA: 6.465545654296875\n",
      "Desvio padrão GPA_GPA: 1.2339450120925903, GPA_not_GPA: 1.2388334274291992\n",
      "Processando classe GRAMINEA_PORTE_BAIXO – GPB...\n",
      "Foram encontrados 1747 imagens e 1747 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1747/1747 [01:44<00:00, 16.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Foreground: torch.Size([15705, 1024]), Total Background: torch.Size([431527, 1024])\n",
      "Original - FG: 15,705, BG: 431,527\n",
      "Calculando distâncias: 7,852 x 7,852 (intra)\n",
      "Calculando distâncias: 7,852 x 215,763 (inter)\n",
      "Distância média GPB_GPB: 5.868480682373047, GPB_not_GPB: 6.166003227233887\n",
      "Desvio padrão GPB_GPB: 0.6617034673690796, GPB_not_GPB: 0.5884485244750977\n",
      "Processando classe MAMONA – MAM...\n",
      "Foram encontrados 1103 imagens e 1103 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1103/1103 [01:18<00:00, 14.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Foreground: torch.Size([10524, 1024]), Total Background: torch.Size([271844, 1024])\n",
      "Original - FG: 10,524, BG: 271,844\n",
      "Calculando distâncias: 5,262 x 5,262 (intra)\n",
      "Calculando distâncias: 5,262 x 135,922 (inter)\n",
      "Distância média MAM_MAM: 7.132387638092041, MAM_not_MAM: 7.052482604980469\n",
      "Desvio padrão MAM_MAM: 4.106703758239746, MAM_not_MAM: 3.2247474193573\n",
      "Processando classe OUTRAS_FOLHAS_LARGAS – OFL...\n",
      "Foram encontrados 2999 imagens e 2999 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2999/2999 [03:38<00:00, 13.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Foreground: torch.Size([28817, 1024]), Total Background: torch.Size([738927, 1024])\n",
      "Original - FG: 28,817, BG: 738,927\n",
      "Calculando distâncias: 14,408 x 14,408 (intra)\n",
      "Calculando distâncias: 14,408 x 369,463 (inter)\n"
     ]
    }
   ],
   "source": [
    "# Iterar sobre as classes e criar um array unico. Inicialmente conterá todas as all_foreground, all_background, mean_dist_classe_classe e mean_dist_classe_not_classe por classe\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, (class_name, dir_class, nickname) in enumerate(classes):\n",
    "    print(f\"Processando classe {class_name} – {nickname}...\")\n",
    "    all_x_foreground, all_x_background = agrupar_tensores(f\"{dataset_path}{dir_class}\")\n",
    "    #mean_dist_classe_classe = pairwise_distances(all_x_foreground).mean()\n",
    "    #mean_dist_classe_not_classe = pairwise_distances(all_x_foreground, all_x_background).mean()\n",
    "    mean_dist_classe_classe, mean_dist_classe_not_classe, std_classe_classe, std_classe_not_classe = calcular_distancias_e_desvio_padrao(all_x_foreground, all_x_background, max_samples_fg=int(0.5*all_x_foreground.shape[0]), max_samples_bg=int(0.5*all_x_background.shape[0]))\n",
    "\n",
    "    results.append({\n",
    "        \"class\": nickname,\n",
    "        \"all_foreground\": all_x_foreground,\n",
    "        \"all_background\": all_x_background,\n",
    "        \"mean_dist_classe_classe\": mean_dist_classe_classe,\n",
    "        \"mean_dist_classe_not_classe\": mean_dist_classe_not_classe,\n",
    "        \"std_classe_classe\": std_classe_classe,\n",
    "        \"std_classe_not_classe\": std_classe_not_classe\n",
    "    })\n",
    "    print(f\"Distância média {nickname}_{nickname}: {mean_dist_classe_classe}, {nickname}_not_{nickname}: {mean_dist_classe_not_classe}\")\n",
    "    print(f\"Desvio padrão {nickname}_{nickname}: {std_classe_classe}, {nickname}_not_{nickname}: {std_classe_not_classe}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e6685",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a3b6d",
   "metadata": {},
   "source": [
    "### Experimento 2: Foreground sera a mask das imagens da classe e Background sera a mask das imagens das outras classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40e72f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-dino (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
