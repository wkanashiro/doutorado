{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6825e50e",
   "metadata": {},
   "source": [
    "# Computing the PCA of a Foreground Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89fa7dd-f5c8-42b7-896c-8287d4e97105",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Let's start by loading some pre-requisites and checking the DINOv3 repository location:\n",
    "- `local` if `DINOV3_LOCATION` environment variable was set to work with a local version of DINOv3 repository;\n",
    "- `github` if the code should be loaded via torch hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebd98e9-13eb-4e7a-a72b-27839dd463d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 location set to /home/lades/computer_vision/wesley/dino-soja/dinov3\n",
      "Loading DINOv3 model dinov3_vitl16...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (rope_embed): RopePositionEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-23): 24 x SelfAttentionBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): SelfAttention(\n",
       "        (qkv): LinearKMaskedBias(in_features=1024, out_features=3072, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (local_cls_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import signal\n",
    "\n",
    "import random\n",
    "\n",
    "DINOV3_GITHUB_LOCATION = \"/home/lades/computer_vision/wesley/dino-soja/dinov3\"\n",
    "\n",
    "if os.getenv(\"DINOV3_LOCATION\") is not None:\n",
    "    DINOV3_LOCATION = os.getenv(\"DINOV3_LOCATION\")\n",
    "else:\n",
    "    DINOV3_LOCATION = DINOV3_GITHUB_LOCATION\n",
    "\n",
    "print(f\"DINOv3 location set to {DINOV3_LOCATION}\")\n",
    "\n",
    "# examples of available DINOv3 models:\n",
    "MODEL_DINOV3_VITS = \"dinov3_vits16\"\n",
    "MODEL_DINOV3_VITSP = \"dinov3_vits16plus\"\n",
    "MODEL_DINOV3_VITB = \"dinov3_vitb16\"\n",
    "MODEL_DINOV3_VITL = \"dinov3_vitl16\"\n",
    "MODEL_DINOV3_VITHP = \"dinov3_vith16plus\"\n",
    "MODEL_DINOV3_VIT7B = \"dinov3_vit7b16\"\n",
    "\n",
    "MODEL_NAME = MODEL_DINOV3_VITL\n",
    "\n",
    "print(f\"Loading DINOv3 model {MODEL_NAME}...\")\n",
    "\n",
    "model = torch.hub.load(\n",
    "    repo_or_dir=DINOV3_LOCATION,\n",
    "    model=MODEL_NAME,\n",
    "    source=\"local\",\n",
    "    weights=\"https://dinov3.llamameta.net/dinov3_vitl16/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiamVkbGhoemY3bnlpYmJ1NnVhdmJ2NGtrIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvZGlub3YzLmxsYW1hbWV0YS5uZXRcLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NTcxNzAwMzh9fX1dfQ__&Signature=YktyLH8aRk9MBqJe9X6LfVwzezdJo51sVHwKx29n7BTo88T8QR4HHDj9kCyi2wVyUsI4qDVXoJWfHDdbbkxiEPqXukQUDVtWq25HG5ODC8JP-%7ENDbRUPQYkPUHER6ssK9WK5K4Nva6EdiBtIKqMm9G3RMh1uFA86wGz4X4FvgDiaRh4aCWsM5jLg3Gsvr1QIFoPpgAO%7EOnYYG-wxj1VRnrY32Wm6OoNw61M96kgfPXNxEFwYphBu2ImJykrvgc0Yea1J2jV3FwECfDmdVftQ3okThpbxwPJ-wsAxDXc2z2-bJaPQowafr8afLKO2hFl57iMiDQELW2DDS9GYgvpw7Q__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=783982447506725\"\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 16\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "SATELLITE_MEAN = (0.430, 0.411, 0.296)\n",
    "SATELLITE_STD = (0.213, 0.156, 0.143)\n",
    "\n",
    "MODEL_TO_NUM_LAYERS = {\n",
    "    MODEL_DINOV3_VITS: 12,\n",
    "    MODEL_DINOV3_VITSP: 12,\n",
    "    MODEL_DINOV3_VITB: 12,\n",
    "    MODEL_DINOV3_VITL: 24,\n",
    "    MODEL_DINOV3_VITHP: 32,\n",
    "    MODEL_DINOV3_VIT7B: 40,\n",
    "}\n",
    "\n",
    "n_layers = MODEL_TO_NUM_LAYERS[MODEL_NAME]\n",
    "    \n",
    "def load_image(path: str) -> Image:\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "# image resize transform to dimensions divisible by patch size\n",
    "def resize_transform(\n",
    "    mask_image: Image,\n",
    "    image_size: int = IMAGE_SIZE,\n",
    "    patch_size: int = PATCH_SIZE,\n",
    ") -> torch.Tensor:\n",
    "    w, h = mask_image.size\n",
    "    h_patches = int(image_size / patch_size)\n",
    "    w_patches = int((w * image_size) / (h * patch_size))\n",
    "    return TF.to_tensor(TF.resize(mask_image, (h_patches * patch_size, w_patches * patch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a069633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using image /home/lades/computer_vision/wesley/dataset/daninhas_multiclasse/DATASET_CARURU/rgb/2025-09-17_TH134_06-05-2025_c_mask_36_34169_21541.jpg\n",
      "Using label /home/lades/computer_vision/wesley/dataset/daninhas_multiclasse/DATASET_CARURU/labels/2025-09-17_TH134_06-05-2025_c_mask_36_34169_21541.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   1, 255], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obter imagem aleatória do dataset\n",
    "dataset_path = \"/home/lades/computer_vision/wesley/dataset/daninhas_multiclasse/DATASET_CARURU/rgb/\"\n",
    "image_uri = f\"{dataset_path}{random.choice(os.listdir(dataset_path))}\"\n",
    "\n",
    "print(f\"Using image {image_uri}\")\n",
    "\n",
    "# Obtém o respectivo label\n",
    "label_uri = image_uri.replace(\"rgb\", \"labels\").replace(\".jpg\", \".png\")\n",
    "label = np.array(Image.open(label_uri))\n",
    "\n",
    "print(f\"Using label {label_uri}\")\n",
    "np.unique(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82cc18e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_tensores(label, image_uri):\n",
    "    image = load_image(image_uri)\n",
    "    image_resized = resize_transform(image)\n",
    "    image_resized_norm = TF.normalize(image_resized, mean=SATELLITE_MEAN, std=SATELLITE_STD)\n",
    "\n",
    "    label_mask = (label == 1).astype(np.float32)\n",
    "    label_mask = signal.convolve2d(label_mask, np.ones((PATCH_SIZE, PATCH_SIZE)), mode='valid')[::PATCH_SIZE, ::PATCH_SIZE]\n",
    "    fg_score_mf = torch.from_numpy(label_mask > 0.5)\n",
    "\n",
    "    h_patches, w_patches = image_resized_norm.shape[1] // PATCH_SIZE, image_resized_norm.shape[2] // PATCH_SIZE\n",
    "    print(f\"Image size: {image_resized_norm.shape}, patches: {h_patches}x{w_patches}\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
    "            feats = model.get_intermediate_layers(image_resized_norm.unsqueeze(0).cuda(), n=range(n_layers), reshape=True, norm=True)\n",
    "            x = feats[-1].squeeze().detach().cpu()\n",
    "            dim = x.shape[0]\n",
    "            x = x.view(dim, -1).permute(1, 0)\n",
    "            \n",
    "    x.shape, fg_score_mf.shape\n",
    "\n",
    "    # Dividir x em foreground e background, considerando a label_mask\n",
    "    foreground_x = x[fg_score_mf.view(-1) > 0.5]\n",
    "    background_x = x[fg_score_mf.view(-1) <= 0.5]\n",
    "\n",
    "    print(f'Foreground: {foreground_x.shape}, Background: {background_x.shape}')\n",
    "\n",
    "    return x, foreground_x, background_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39b22b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_pca(label, image_uri): \n",
    "    # Cria a máscara de foreground (tudo que estiver no fundo preto que seja diferente de branco)\n",
    "    label_mask = (label == 1).astype(np.float32)\n",
    "    label_mask = signal.convolve2d(label_mask, np.ones((PATCH_SIZE, PATCH_SIZE)), mode='valid')[::PATCH_SIZE, ::PATCH_SIZE]\n",
    "    fg_score_mf = torch.from_numpy(label_mask > 0.5)\n",
    "\n",
    "    image = load_image(image_uri)\n",
    "    image_resized = resize_transform(image)\n",
    "    image_resized_norm = TF.normalize(image_resized, mean=SATELLITE_MEAN, std=SATELLITE_STD)\n",
    "\n",
    "    h_patches, w_patches = image_resized_norm.shape[1] // PATCH_SIZE, image_resized_norm.shape[2] // PATCH_SIZE\n",
    "    print(f\"Image size: {image_resized_norm.shape}, patches: {h_patches}x{w_patches}\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
    "            feats = model.get_intermediate_layers(image_resized_norm.unsqueeze(0).cuda(), n=range(n_layers), reshape=True, norm=True)\n",
    "            x = feats[-1].squeeze().detach().cpu()\n",
    "            dim = x.shape[0]\n",
    "            x = x.view(dim, -1).permute(1, 0)\n",
    "            \n",
    "    fg_patches = x\n",
    "\n",
    "    pca = PCA(n_components=3, whiten=True)\n",
    "    pca.fit(fg_patches)\n",
    "\n",
    "    # apply the PCA, and then reshape\n",
    "    projected_image = torch.from_numpy(pca.transform(x.numpy())).view(h_patches, w_patches, 3)\n",
    "\n",
    "    # multiply by 2.0 and pass through a sigmoid to get vibrant colors \n",
    "    projected_image = torch.nn.functional.sigmoid(projected_image.mul(2.0)).permute(2, 0, 1)\n",
    "\n",
    "    # mask the background using the fg_score_mf\n",
    "    projected_foreground_image = projected_image * (fg_score_mf.unsqueeze(0) > 0.5)\n",
    "    \n",
    "    projected_background_image = projected_image * (fg_score_mf.unsqueeze(0) <= 0.5)\n",
    "\n",
    "    return projected_image, projected_foreground_image, projected_background_image\n",
    "\n",
    "def plotar_imagens(image_uri, projected_image, projected_foreground_image, label_mask, label_uri):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.imshow(np.array(Image.open(image_uri)))\n",
    "    plt.title(\"Imagem original\")\n",
    "    plt.subplot(1, 5, 2)\n",
    "    plt.imshow(projected_image.permute(1, 2, 0))\n",
    "    plt.title(\"PCA da imagem projetada\")\n",
    "    plt.subplot(1, 5, 3)\n",
    "    plt.imshow(projected_foreground_image.permute(1, 2, 0))\n",
    "    plt.title(\"PCA da imagem projetada + mask\")\n",
    "    plt.subplot(1, 5, 4)\n",
    "    plt.imshow(np.array(Image.fromarray(label_mask)))\n",
    "    plt.title(\"PCA da mask label (GT)\")\n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.imshow(Image.open(label_uri))\n",
    "    plt.title(\"Label (GT)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Exibir label_mask com os valores de cada bloquinho\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(label_mask)\n",
    "    for i in range(label_mask.shape[0]):\n",
    "        for j in range(label_mask.shape[1]):\n",
    "            plt.text(j, i, f\"{label_mask[i, j]:.1f}\", ha='center', va='center', color='white')\n",
    "    plt.show()\n",
    "\n",
    "    # Exibir projected_image com os valores de cada bloquinho\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(projected_image.permute(1, 2, 0))\n",
    "    for i in range(projected_image.shape[1]):\n",
    "        for j in range(projected_image.shape[2]):\n",
    "            plt.text(j, i, f\"{projected_image[0, i, j]:.1f}\", ha='center', va='center', color='white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936f236",
   "metadata": {},
   "source": [
    "### Experimento\n",
    "Neste experimento, irei considerar o mask label da respectiva imagem, para separar os patches do PCA da imagem que estiverem dentro da mask label da imagem. Isso sera o foreground e o restante, background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894d6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([31, 1024]), Background: torch.Size([225, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([130, 1024]), Background: torch.Size([126, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([63, 1024]), Background: torch.Size([193, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([5, 1024]), Background: torch.Size([251, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([43, 1024]), Background: torch.Size([213, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([7, 1024]), Background: torch.Size([249, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([84, 1024]), Background: torch.Size([172, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([5, 1024]), Background: torch.Size([251, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([135, 1024]), Background: torch.Size([121, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([98, 1024]), Background: torch.Size([158, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([22, 1024]), Background: torch.Size([234, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([105, 1024]), Background: torch.Size([151, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([129, 1024]), Background: torch.Size([127, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([17, 1024]), Background: torch.Size([239, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([95, 1024]), Background: torch.Size([161, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([64, 1024]), Background: torch.Size([192, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([93, 1024]), Background: torch.Size([163, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([22, 1024]), Background: torch.Size([234, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([38, 1024]), Background: torch.Size([218, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([25, 1024]), Background: torch.Size([231, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([143, 1024]), Background: torch.Size([113, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([160, 1024]), Background: torch.Size([96, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([8, 1024]), Background: torch.Size([248, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([19, 1024]), Background: torch.Size([237, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([72, 1024]), Background: torch.Size([184, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([122, 1024]), Background: torch.Size([134, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([61, 1024]), Background: torch.Size([195, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([8, 1024]), Background: torch.Size([248, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([188, 1024]), Background: torch.Size([68, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([27, 1024]), Background: torch.Size([229, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([76, 1024]), Background: torch.Size([180, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([106, 1024]), Background: torch.Size([150, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([24, 1024]), Background: torch.Size([232, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([52, 1024]), Background: torch.Size([204, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([13, 1024]), Background: torch.Size([243, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([21, 1024]), Background: torch.Size([235, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([52, 1024]), Background: torch.Size([204, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([183, 1024]), Background: torch.Size([73, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([62, 1024]), Background: torch.Size([194, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([25, 1024]), Background: torch.Size([231, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([8, 1024]), Background: torch.Size([248, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([3, 1024]), Background: torch.Size([253, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([86, 1024]), Background: torch.Size([170, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([31, 1024]), Background: torch.Size([225, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([85, 1024]), Background: torch.Size([171, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([14, 1024]), Background: torch.Size([242, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([23, 1024]), Background: torch.Size([233, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([102, 1024]), Background: torch.Size([154, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([92, 1024]), Background: torch.Size([164, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([12, 1024]), Background: torch.Size([244, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([126, 1024]), Background: torch.Size([130, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([14, 1024]), Background: torch.Size([242, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([18, 1024]), Background: torch.Size([238, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([4, 1024]), Background: torch.Size([252, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([21, 1024]), Background: torch.Size([235, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([9, 1024]), Background: torch.Size([247, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([3, 1024]), Background: torch.Size([253, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([11, 1024]), Background: torch.Size([245, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([19, 1024]), Background: torch.Size([237, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([41, 1024]), Background: torch.Size([215, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([78, 1024]), Background: torch.Size([178, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([26, 1024]), Background: torch.Size([230, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([21, 1024]), Background: torch.Size([235, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([41, 1024]), Background: torch.Size([215, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([106, 1024]), Background: torch.Size([150, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([39, 1024]), Background: torch.Size([217, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([123, 1024]), Background: torch.Size([133, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([154, 1024]), Background: torch.Size([102, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([33, 1024]), Background: torch.Size([223, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([90, 1024]), Background: torch.Size([166, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([102, 1024]), Background: torch.Size([154, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([5, 1024]), Background: torch.Size([251, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([15, 1024]), Background: torch.Size([241, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([123, 1024]), Background: torch.Size([133, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([5, 1024]), Background: torch.Size([251, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([66, 1024]), Background: torch.Size([190, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([41, 1024]), Background: torch.Size([215, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([17, 1024]), Background: torch.Size([239, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([89, 1024]), Background: torch.Size([167, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([122, 1024]), Background: torch.Size([134, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([19, 1024]), Background: torch.Size([237, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([122, 1024]), Background: torch.Size([134, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([71, 1024]), Background: torch.Size([185, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([13, 1024]), Background: torch.Size([243, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([4, 1024]), Background: torch.Size([252, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([58, 1024]), Background: torch.Size([198, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([68, 1024]), Background: torch.Size([188, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([23, 1024]), Background: torch.Size([233, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([6, 1024]), Background: torch.Size([250, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([23, 1024]), Background: torch.Size([233, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([30, 1024]), Background: torch.Size([226, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([19, 1024]), Background: torch.Size([237, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([15, 1024]), Background: torch.Size([241, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([27, 1024]), Background: torch.Size([229, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([20, 1024]), Background: torch.Size([236, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([17, 1024]), Background: torch.Size([239, 1024])\n",
      "Image size: torch.Size([3, 256, 256]), patches: 16x16\n",
      "Foreground: torch.Size([19, 1024]), Background: torch.Size([237, 1024])\n"
     ]
    }
   ],
   "source": [
    "dataset_path_rgb = \"/home/lades/computer_vision/wesley/dataset/daninhas_multiclasse/DATASET_CARURU/rgb/\"\n",
    "dataset_path_label = \"/home/lades/computer_vision/wesley/dataset/daninhas_multiclasse/DATASET_CARURU/labels/\" # Extensão png\n",
    "\n",
    "lista_imagens = [f\"{dataset_path_rgb}{file}\" for file in os.listdir(dataset_path_rgb) if file.endswith('.jpg')]\n",
    "lista_labels = [f\"{dataset_path_label}{file.split('.')[0]}.png\" for file in os.listdir(dataset_path_rgb) if file.endswith('.jpg')]\n",
    "\n",
    "len(lista_imagens), len(lista_labels)\n",
    "\n",
    "x_caruru = []\n",
    "x_not_caruru = []\n",
    "\n",
    "for image_uri, label_uri in zip(lista_imagens, lista_labels):\n",
    "    label = np.array(Image.open(label_uri))\n",
    "    x, foreground_x, background_x = extrair_tensores(label, image_uri)\n",
    "    x_caruru.append(foreground_x)\n",
    "    x_not_caruru.append(background_x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff25b567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5205, 1024]), torch.Size([19627, 1024]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_x_caruru = torch.cat(x_caruru, dim=0)\n",
    "all_x_not_caruru = torch.cat(x_not_caruru, dim=0)\n",
    "\n",
    "all_x_caruru.shape, all_x_not_caruru.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e853503c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.208234, 5.623066)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcular distância média entre os pontos da própria classe (mean_dist_caruru_caruru e mean_dist_caruru_not_caruru)\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "mean_dist_caruru_caruru = pairwise_distances(all_x_caruru).mean()\n",
    "mean_dist_caruru_not_caruru = pairwise_distances(all_x_caruru, all_x_not_caruru).mean()\n",
    "\n",
    "mean_dist_caruru_caruru, mean_dist_caruru_not_caruru\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-dino (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
